[
  {
    "title": "SpeechCARE: dynamic multimodal modeling for cognitive screening in diverse linguistic and speech task contexts",
    "authors": "Hossein Azadmaleki, Yasaman Haghbin, Sina Rashidi, Mohammad Javad Momeni Nezhad, Ali Zolnour & Maryam Zolnoori",
    "venue": " npj digital medicine ",
    "year": 2025,
    "month": 11,
    "type": "Journal Article",
    "link": "https://www.nature.com/articles/s41746-025-02026-x",
    "image": "/images/publications/speechCARE.png",
    "challenge": "Early detection of cognitive impairment from speech is difficult because available datasets are small, noisy, multilingual, and collected from inconsistent speech tasks. Traditional acoustic features and general-purpose models often fail to capture subtle linguistic and acoustic cues needed to identify Mild Cognitive Impairment (MCI) and Alzheimer’s disease.",
    "solution": "SpeechCARE introduces a dynamic multimodal transformer pipeline that integrates: <ul><li>Advanced preprocessing (LLM-based noise/anomaly detection, speech-task identification, ASR transcription). </li> <li>Multimodal modeling combining acoustic (mHuBERT), linguistic (mGTE), and demographic features.</li> <li>A novel Adaptive Gating Fusion (AGF) mechanism that weights modalities differently depending on the speech task.</li></ul> This architecture captures long-range speech patterns and adapts across languages and task types.",
    "result": "72.11% average F1-score and 86.83% AUC on the NIA PREPARE test set (English, Spanish, Mandarin). <ul> <li>Improved MCI detection through threshold optimization. </li> <li>Top-tier performance in the NIA challenge, earning special recognition. <li>Strong external generalization:<ul class='list-disc pl-5'><li>85.08% F1-score on ADReSSo 2021.</li><li>92.67% AUC / 85.47% F1-score when fine-tuned on Mandarin (Chou Corpus).</li></ul></li></ul>"
  },
  {
    "title": "LLMCARE: early detection of cognitive impairment via transformer models enhanced by LLM-generated synthetic data",
    "authors": "Ali Zolnour, Hossein Azadmaleki, Yasaman Haghbin, Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sina Rashidi, Masoud Khani, AmirSajjad Taleban, Samin Mahdizadeh Sani, Maryam Dadkhah, James M. Noble, Suzanne Bakken, Yadollah Yaghoobzadeh, Abdol-Hossein Vahabie, Masoud Rouhizadeh, Maryam Zolnoori",
    "venue": "Frontiers in Artificial Intelligence",
    "year": 2025,
    "month": 11,
    "type": "Journal Article",
    "link": "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2025.1669896/full",
    "image": "/images/publications/LLMCARE.jpg",
    "challenge": "Speech-based Alzheimer’s screening is limited by scarce labeled clinical speech data, inconsistent model performance across fine-tuning strategies, and poor generalizability across populations and datasets. Prior synthetic data approaches also show mixed results, making it difficult to build reliable and scalable early-screening tools.",
    "solution": "LLMCARE introduces a multi-component Alzheimer’s screening pipeline that integrates:<ul><li><strong>Transformer-based linguistic modeling:</strong> Systematic evaluation of 10 transformer models (general-purpose and clinical-domain) with multiple fine-tuning strategies to capture disfluencies, syntactic irregularities, and semantic drift common in cognitive impairment.</li> <li><strong>Handcrafted linguistic biomarkers:</strong> 110 features covering lexical richness, syntactic complexity, discourse fluency, and psycholinguistic categories to enhance interpretability and complement deep embeddings.</li> <li><strong>Late-fusion architecture:</strong> A learnable weighted fusion layer combining transformer embeddings with handcrafted features to improve robustness and generalization.</li> <li><strong>LLM-generated synthetic speech:</strong> Fine-tuned LLaMA, MedAlpaca, Ministral, and GPT-4o models generate label-conditioned transcripts that emulate cognitively healthy vs. impaired speech for controlled data augmentation.</li> <li><strong>LLMs as classifiers:</strong> Evaluation of unimodal (text-only) and multimodal (audio+text) LLMs in zero-shot and fine-tuned settings to benchmark modern generative models for dementia detection.</li> <li><strong>External generalization:</strong> Validation on the DementiaBank Delaware MCI cohort demonstrates transferability beyond the ADReSSo benchmark and supports clinical screening potential.</li></ul>This architecture strengthens linguistic signal extraction, improves classification performance through aligned synthetic augmentation, and generalizes to early-stage cognitive impairment.",
    "result": "LLMCARE achieved:<ul> <li><strong>ADReSSo 2021:</strong> F1 = <strong>83.32 ± 2.78</strong>, AUC = <strong>89.48 ± 4.40</strong> using the fusion model.</li> <li><strong>Delaware MCI Cohort:</strong> F1 = <strong>72.82</strong>, AUC = <strong>69.57</strong> with 1× MedAlpaca-7B augmentation.</li> <li><strong>Augmentation effects:</strong> MedAlpaca-7B improved performance up to 2× synthetic data (peak F1 ≈ <strong>85.7</strong>).</li> <li><strong>Transformer evaluation:</strong> BERT achieved the highest F1 (82.76 ± 4.51) among general-purpose models.</li> <li><strong>External LLM benchmark:</strong> LLaMA, GPT-4o, and Phi-4 multimodal models show strong viability for dementia detection.</li> </ul> Overall, LLMCARE demonstrates that combining transformer models, handcrafted linguistic features, and aligned LLM-generated narratives yields strong and generalizable performance for early-stage cognitive impairment screening."
  },
  {
    "title": "SpeechDETECT: An Explainable Automated Speech Processing Pipeline for Early Detection of Neurological and Health Changes",
    "authors": "Maryam Zolnoori, Elyas Esmaeili, Mehdi Naserian, Ali Zolnour, Sina Rashidi, Tahoura Morovati, Hossein Azadmaleki, James M Noble, Margaret V McDonald",
    "venue": "Health Information Science and Systems",
    "year": 2025,
    "month": 10,
    "type": "Journal Article",
    "image": "/images/publications/speechDETECT.jpg",
    "challenge": "Early cognitive impairment remains widely underdiagnosed, particularly in community and underserved settings. While biomarkers such as MRI and CSF offer high accuracy, they are costly and inaccessible for  large-scale screening. Existing speech-based pipelines rely on limited handcrafted features or black-box  models, lack clinical interpretability, and struggle to generalize across diverse speech tasks and short real-world recordings.",
    "solution": "SpeechDETECT is a fully automated and explainable speech-processing pipeline designed to capture fine-grained acoustic and temporal markers of cognitive impairment. The system integrates: <ul> <li>Noise reduction and amplitude normalization for robust preprocessing.</li> <li>An eight-domain voice-analysis framework covering frequency, spectral, voice quality, loudness, complexity, rhythm, fluency, and production dynamics.</li> <li>50 ms segment-level acoustic extraction plus temporal features computed using WhisperX and phoneme-level alignment.</li> <li>Feature selection via JMIM, LASSO, LassoNet, and PCA to avoid overfitting.</li> <li>Machine learning classifiers with SHAP explainability to identify clinically meaningful vocal markers.</li> </ul> The pipeline was evaluated on both the structured DementiaBank Pitt dataset and the multi-task NIA PREPARE Phase 2 dataset.",
    "result": "SpeechDETECT achieved strong and interpretable prediction performance: <ul> <li><strong>Pitt test set:</strong> F1 = <strong>0.81</strong>, AUC = <strong>0.80</strong>, outperforming six major acoustic toolkits.</li> <li><strong>PREPARE test set:</strong> F1 ≈ <strong>0.67</strong>, AUC = <strong>0.70</strong>, showing good generalizability across diverse short recordings.</li> <li><strong>Cumulative gains:</strong> Top 40% of ranked participants captured ~70% of impaired cases in Pitt and ~63% in PREPARE.</li> <li><strong>Explainability (SHAP):</strong> Key predictive markers included hesitation rate, pause ratio, spectral flattening (MFCC/LTAS), jitter, shimmer, NHR, and reduced utterance complexity.</li> </ul> These findings demonstrate that SpeechDETECT provides accurate, interpretable, and scalable speech-based screening for early cognitive impairment."
  },
  {
    "title": "TransformerCARE: A Novel Speech Analysis Pipeline Using Transformer-Based Models and Audio Augmentation Techniques for Cognitive Impairment Detection",
    "authors": "Hossein Azadmaleki, Ali Zolnour, Sina Rashidi, James M. Noble, Julia Hirschberg, Elyas Esmaeili, Tahoura Morovati, Maryam Zolnoori",
    "venue": "International Journal of Medical Informatics",
    "year": 2025,
    "month": 10,
    "type": "Journal Article",
    "image": "/images/publications/transformerCARE.jpg",
    "challenge": "Early cognitive impairment remains hard to detect because more than half of dementia cases go undiagnosed until late stages. Existing biomarkers such as MRI and CSF are costly and invasive, while current speech pipelines rely on hand-crafted features or small deep-learning models that miss subtle acoustic cues. Transformer models offer promise but struggle with long audio recordings, limited datasets, and computational constraints.",
    "solution": "TransformerCARE is a transformer-based speech analysis pipeline designed for scalable early detection of cognitive impairment. The system integrates: <ul> <li><strong>Preprocessing</strong> with noise reduction and normalization.</li> <li><strong>Speech segmentation</strong> optimized at 14-second segments with 25% overlap.</li> <li><strong>Advanced transformer models</strong> (Wav2vec2, HuBERT, WavLM, DistilHuBERT) with fine-tuning.</li> <li><strong>Embed-based aggregation</strong> for richer subject-level representations.</li> <li><strong>Audio augmentation</strong> using pitch shifting, noise injection, time shifting, and frequency masking—preserving critical acoustic cues.</li> <li><strong>Provider–patient interaction speech</strong> to capture real communication cues during tasks.</li> </ul> This unified pipeline enhances contextual speech modeling and improves model generalizability.",
    "result": "TransformerCARE achieved strong and clinically meaningful performance: <ul> <li><strong>Baseline (HuBERT):</strong> AUC = <strong>81.80</strong>, F1 = <strong>79.31</strong>.</li> <li><strong>Frequency masking augmentation:</strong> AUC = <strong>86.11</strong>, F1 = <strong>84.63</strong> (≈5% improvement).</li> <li><strong>Provider speech effect:</strong> Removing provider speech reduces performance by ~2.3%.</li> <li><strong>Cross-validation robustness:</strong> AUC = <strong>84.8–93.9</strong>, F1 = <strong>83.5–90.1</strong> across folds.</li> <li><strong>Error analysis:</strong> Misclassified subjects show deviations in pause length, jitter, entropy, and speech rate.</li> </ul> These findings demonstrate that TransformerCARE provides a high-performing, scalable, and noninvasive tool for early cognitive impairment screening."
  },
  {
    "title": "ADscreen: A speech processing-based screening system for automatic identification of patients with Alzheimer’s disease and related dementia",
    "authors": "Maryam Zolnoori, Ali Zolnour,  Maxim Topaz",
    "venue": "Artificial Intelligence In Medicine",
    "year": 2023,
    "month": 7,
    "type": "Journal Article",
    "image": "/images/publications/ADScreen.jpg",
    "link": "https://pubmed.ncbi.nlm.nih.gov/37673583/",
    "challenge": "Alzheimer’s disease and related dementias affect millions, yet more than half of cases remain undiagnosed due to limited access to clinical biomarkers and constrained clinical time. Existing speech-based systems typically analyze only acoustic or linguistic cues and overlook deeper aspects of speech such as phonetic motor planning, semantic organization, and psycholinguistic changes—all early indicators of cognitive decline.",
    "solution": "ADscreen introduces an integrated speech-processing pipeline that models three major components of patient speech: <ul> <li><strong>Phonetic motor planning:</strong> Detailed acoustic analysis of fluency, spectral features, intensity, voice quality, and rhythmic structure.</li> <li><strong>Semantic & syntactic language organization:</strong> Measures of lexical richness, repetition, pausing behavior, syntactic patterns, and contextual embeddings from DistilBERT.</li> <li><strong>Psycholinguistic cues:</strong> Emotional and cognitive indicators captured using GeMAPS acoustic parameters and LIWC semantic features.</li> </ul> These components are combined using JMIM feature selection and fused in a multimodal ML architecture for robust ADRD detection.",
    "result": "ADscreen achieved state-of-the-art performance on the DementiaBank Cookie-Theft dataset: <ul><li><strong>AUC-ROC:</strong> 93.89</li><li><strong>Accuracy:</strong> 90.14</li><li><strong>F1-score:</strong> 89.55</li></ul>Fusion of acoustic, linguistic, psycholinguistic, and transformer-based features substantiallyoutperformed unimodal models, demonstrating ADscreen’s strong potential as a clinical screening tool for early detection of cognitive impairment."
  },
  {
    "title": "Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies",
    "authors": "Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sepehr Karimi, Sina Rashidi, Ali Zolnour, Maryam Dadkhah, Yasaman Haghbin, Hossein AzadMaleki, Maryam Zolnoori",
    "venue": "JMIR",
    "year": 2025,
    "month": 8,
    "type": "Under Review",
    "link": "https://arxiv.org/abs/2509.03525",
    "image": "/images/publications/LLM.png",
    "challenge": "More than half of adults with Alzheimer’s disease and related dementias remain undiagnosed, and existing speech-based screening pipelines rely on hand-crafted features or task-specific transformers that require large labeled datasets. The impact of different large language model(LLM) adaptation strategies—in-context learning, reasoning prompts, fine-tuning methods, and multimodal audio–text integration—on cognitive screening accuracy has not been systematically evaluated.",
    "solution": "This work systematically evaluates LLM-based adaptation strategies for detecting cognitive impairment from DementiaBank Cookie-Theft transcripts: <ul><li><strong>In-context learning (ICL):</strong> Few-shot prompting with four demonstration selection rules (most-similar, least-similar, class-centroid prototypes, random).</li> <li><strong>Reasoning-augmented prompting:</strong> Self- and teacher-generated rationales, self-consistency voting, and Tree-of-Thought expert reasoning for smaller models.</li> <li><strong>Parameter-efficient fine-tuning:</strong> Token-level supervised fine-tuning via LoRA and an alternative lightweight classification head attached to LLM hidden states.</li> <li><strong>Multimodal audio–text models:</strong> GPT-4o mini, Qwen-2.5 Omni, and Phi-4 Multimodal combining speech and transcripts.</li></ul> The study compares open-weight (LLaMA, Ministral, MedAlpaca, DeepSeek) and commercial(GPT-4o, Gemini 2.0 Flash) models to identify practical, scalable strategies for speech-based ADRD screening.",
    "result": "The evaluation reveals clear patterns across adaptation strategies:<ul><li><strong>In-context learning:</strong> Class-centroid (prototype) demonstrations achieve the best ICL performance across most models (F1 up to <strong>0.81</strong>), with most-similar as the second-best strategy.</li> <li><strong>Reasoning:</strong> Teacher-generated rationales and expert-role Tree-of-Thought mainly benefit smaller models (e.g., LLaMA-8B zero-shot F1 improves from <strong>0.65</strong> to <strong>0.71</strong>).</li> <li><strong>Token-level fine-tuning:</strong> Yields the strongest overall performance (LLaMA-3B F1 = <strong>0.83</strong>, AUC = <strong>0.91</strong>; LLaMA-70B F1 = <strong>0.83</strong>, AUC = <strong>0.86</strong>; GPT-4o F1 = <strong>0.80</strong>, AUC = <strong>0.87</strong>).</li> <li><strong>Classification-head fine-tuning:</strong> Dramatically improves models that perform poorly with token supervision (MedAlpaca-7B F1 increases from <strong>0.06</strong> to <strong>0.82</strong>, AUC to <strong>0.92</strong>), but often degrades already strong models.</li><li><strong>Multimodal models:</strong> Fine-tuned Phi-4 Multimodal reaches F1 = <strong>0.80</strong> (CI) and <strong>0.75</strong> (CN), but does not surpass the best text-only systems; GPT-4o mini and Qwen-2.5 Omni show class bias and limited benefit from fine-tuning.</li> </ul> Overall, well-adapted open-weight text LLMs can match or exceed commercial systems for speech-based cognitive screening, while current multimodal models require better audio–text alignment and larger training corpora."
  },
  {
    "title": "Detecting Mild Cognitive Impairment Using Follow-Up Call Speech and Electronic Health Record Data in Home Health Care Settings",
    "authors": "Maryam Zolnoori, Ali Zolnour, Sina Rashidi, Ian Spens, Yasaman Haghbin, Sasha Vergez, Grace Flaherty, Nicole Onorato, Felix Vasquez, James M. Noble and Margaret McDonald",
    "venue": "Journal of Gerontological Nursing",
    "year": 2025,
    "month": 12,
    "type": "Journal Article",
    "link": "",
    "image": "/images/publications/Phone_Call.jpg",
    "challenge": "Older adults receiving home healthcare face high rates of undiagnosed mild cognitive impairment (MCI)—over 60% of early symptoms go unrecognized because: <ul><li>Early cognitive changes (e.g., slowed speech, memory lapses) are rarely documented in EHRs.</li> <li>Existing EHR-based risk models show only moderate sensitivity.</li> <li>In-person cognitive testing is burdensome, resource-intensive, and often infeasible in diverse home-care settings.</li></ul> This results in missed opportunities for early intervention among underserved, multimorbid, aging populations.",
    "solution": "This study developed a speech-based screening algorithm that analyzes brief, semi-structured follow-up phone calls between home-health patients and nurses. The system integrates: <ul> <li><strong>Acoustic biomarkers</strong> capturing pauses, pitch variability, prosody, fluency, and rhythm.</li> <li><strong>Linguistic features</strong> from transformer models (BERT) reflecting semantic and syntactic disorganization.</li> <li><strong>Structured EHR variables (OASIS)</strong> describing comorbidities, function, and utilization patterns.</li> <li><strong>Multimodal machine-learning fusion</strong> combining speech and clinical data to enhance predictive accuracy.</li> </ul> This approach leverages routine communication already present in home healthcare, enabling scalable, low-burden cognitive surveillance.",
    "result": "The study evaluated 114 participants and found: <ul> <li><strong>Follow-up call speech alone:</strong> AUC = <strong>81.09</strong>, with the highest sensitivity (92.31%).</li> <li><strong>CDR interview speech:</strong> AUC = <strong>80.67</strong>, with the highest specificity (93.33%).</li> <li><strong>OASIS clinical data alone:</strong> AUC = <strong>78.53</strong>.</li> <li><strong>Multimodal (OASIS + follow-up speech):</strong> AUC = <strong>91.03</strong>, F1 = <strong>81.48</strong>, the best-performing model.</li> </ul> Combining routine follow-up call speech with structured EHR data yields a powerful screening tool for early detection of cognitive impairment in home healthcare."
  },
  {
    "title": "Multimodal Attention Fusion of Speech and EHR Data for Early Detection of Cognitive Decline in Home Healthcare",
    "authors": "Yasaman Haghbin*, Sina Rashidi*, Margaret McDonald, Maryam Zolnoori",
    "venue": "ICASSP 2026",
    "year": 2026,
    "month": 4,
    "type": "Under Review",
    "image": "/images/publications/BIMS.jpg",
    "challenge": "Early cognitive decline is frequently missed in home healthcare because early symptoms are rarely captured in EHRs, existing EHR-based models show only moderate performance, and most prior speech studies rely on structured laboratory tasks that do not reflect subtle, real-world decline. Natural patient–nurse speech, although rich with cognitive information, has been underused and never fully integrated with EHR data.",
    "solution": "This study introduces a multimodal attention-based fusion framework that integrates: <ul> <li><strong>Structured EHR variables</strong> capturing clinical, functional, demographic, and behavioral data.</li> <li><strong>Check-in phone call speech</strong> providing semi-structured acoustic and linguistic cues.</li> <li><strong>Patient–nurse verbal communication</strong> offering rich, naturalistic indicators of cognitive function.</li> <li><strong>SpeechDETECT acoustic features</strong> reflecting prosody, rhythm, voice quality, and clarity.</li> <li><strong>Transformer-based linguistic embeddings</strong> (BERT, ClinicalBERT, BioMedBERT) capturing semantic and syntactic anomalies.</li> <li><strong>Attention-based bottleneck fusion</strong> to efficiently combine cross-modal information.</li></ul> This framework provides a scalable, minimally invasive method for early cognitive impairment detection.",
    "result": "Evaluation on 175 participants showed strong, stepwise gains when speech data were added:<ul><li><strong>EHR only:</strong> AUC = <strong>0.74</strong>, F1 = <strong>58.22</strong>.</li><li><strong>EHR + first check-in call:</strong> AUC = <strong>0.76</strong>, F1 = <strong>63.80</strong>.</li><li><strong>EHR + one patient–nurse conversation:</strong> AUC = <strong>0.93</strong>, F1 = <strong>80.78</strong>.</li><li><strong>EHR + two patient–nurse conversations:</strong> AUC = <strong>0.94</strong>, F1 = <strong>84.91</strong>.</li><li><strong>Best model (EHR + first check-in call + both conversations):</strong> AUC = <strong>0.98</strong>, F1 = <strong>90.51</strong>.</li></ul>Natural speech—especially patient–nurse conversations—provided the strongest predictive signal, while EHR data added complementary clinical context. This multimodal method offers a powerful, non-invasive tool for early cognitive decline detection in home healthcare."
  },
  {
    "title": "Leveraging Text-To-Speech and Voice Conversion as Data Augmentation for Alzheimer's Disease Detection from Spontaneous Speech",
    "authors": "Sina Rashidi, Yasaman Haghbin, Hossein AzadMaleki, Ali Zolnour, Maryam Zolnoori",
    "venue": "ICASSP 2026",
    "year": 2026,
    "month": 4,
    "type": "Under Review",
    "image": "/images/publications/SpeechCURA.jpg",
    "challenge": "Alzheimer’s disease detection from speech is limited by scarce patient audio data, which restricts model training and weakens generalizability. This scarcity arises because: Collecting clinical speech is difficult due to privacy constraints, clinician burden, and inconsistent recording environments. Existing speech datasets (e.g., DementiaBank, ADReSSo) are small and lack demographic and acoustic diversity, reducing robustness. Conventional augmentation (e.g., SpecAugment) introduces limited variability, altering acoustics but not linguistic cues. These limitations hinder the development of accurate, clinically deployable ADRD screening models.",
    "solution": "This study introduces two generative pipelines that expand both acoustic and linguistic diversity: <ul> <li><strong>Text-to-Speech (TTS) pipeline:</strong> Fine-tuned LLaMA-3.1-8B and medGemma-27B models generate diagnosis-specific synthetic transcripts, which are converted to audio using SparkTTS with diagnosis-matched speaker embeddings.</li> <li><strong>Voice Conversion (VC) pipeline:</strong> Uses OpenVoice to convert each real speaker’s voice into another speaker’s profile while preserving linguistic content, increasing acoustic diversity.</li> <li><strong>Model evaluation:</strong> Both pipelines are tested with SpeechCARE-AGF (multimodal) and SpeechCARE-Whisper (acoustic-only) transformer architectures.</li></ul> Together, these pipelines create rich, clinically relevant synthetic speech to strengthen model training.",
    "result": "Generative augmentation consistently improved diagnostic performance: <ul> <li><strong>Baseline (SpeechCARE-Whisper):</strong> Micro-F1 = <strong>80.2</strong>, ADRD-F1 = <strong>82.9</strong>.</li> <li><strong>TTS augmentation (best overall):</strong> Micro-F1 = <strong>90.1</strong>, ADRD-F1 = <strong>90.4</strong>, achieving state-of-the-art performance on spontaneous speech.</li> <li><strong>Voice conversion:</strong> Micro-F1 = <strong>90.1</strong>, ADRD-F1 = <strong>90.1</strong>.</li> <li><strong>SpecAugment:</strong> mixed impact—beneficial for Whisper but reduced AGF performance.</li> </ul> These findings show that generative synthesis adds meaningful acoustic and linguistic variation, outperforming conventional augmentations and enabling more accurate, robust Alzheimer’s detection."
  },
  {
    "title": "Beyond EHR data: Leveraging Natural Language Processing and ML to Uncover Cognitive Insights from Patient-Nurse Verbal Communications",
    "authors": "Maryam Zolnoori, Ali Zolnour, Sasha Vergez, Sridevi Sridharan, Ian Spens, Maxim Topaz, James M. Noble, Suzanne Bakken, ulia Hirschberg, Kathryn Bowles, Nicole Onorato, Margaret V. McDonald",
    "venue": "Journal of the American Medical Informatics Association (JAMIA)",
    "year": 2025,
    "type": "Journal Article",
    "image": "/images/publications/EHR.png",
    "link": "https://academic.oup.com/jamia/article-abstract/32/2/328/7922610",
    "challenge": "Mild cognitive impairment and early-stage dementia are highly prevalent in home healthcare, yet more than half of affected patients remain undiagnosed. Conventional screening algorithms based only on electronic health record (EHR) data capture limited early cognitive cues and achieve only moderate performance. Routine patient–nurse encounters, which are rich in linguistic and interactional information, are rarely analyzed systematically, leaving a critical data stream underused for early cognitive decline detection.",
    "solution": "This study develops an NLP- and ML-based screening pipeline that leverages routine audio-recorded patient–nurse verbal communication in home healthcare and combines it with EHR data: <ul> <li><strong>Patient speech analysis:</strong> Domain-specific linguistic features (lexical richness, syntactic complexity, semantic coherence) and psycholinguistic cues (LIWC) are extracted from patient utterances, along with contextual embeddings from DistilBERT.</li> <li><strong>Interaction cues:</strong> Turn-taking patterns, dialogue interactivity, speaking-time statistics, and silence durations are computed from patient–nurse dialogues to capture social and communicative changes linked to cognitive decline.</li><li><strong>Nurse language analysis:</strong> BioClinicalBERT embeddings and LIWC features from nurse utterances model how clinicians adapt their language when interacting with cognitively impaired patients.</li> <li><strong>EHR representation:</strong> BioClinicalBERT-encoded clinical notes and UMLS-based concept extraction are combined with structured demographics and medical history.</li> <li><strong>Multisource ML classifiers:</strong> Features are selected with Joint Mutual Information Maximization (JMIM) and used to train Logistic Regression, XGBoost, and SVM models on (a) patient speech, (b) patient + nurse speech, (c) EHR data alone, and (d) late-fusion integration of communication and EHR features.</li></ul> This integrated framework moves beyond EHR-only screening by incorporating rich conversational signals from home healthcare visits.",
    "result": "The pipeline was evaluated on 125 audio-recorded encounters from 47 patients: <ul> <li><strong>Patient speech only:</strong> SVM-Linear achieved F1 = <strong>75%</strong>, F2 = <strong>77.32%</strong>, AUC-ROC = <strong>75.94</strong>.</li> <li><strong>Patient + nurse speech:</strong> Adding nurse language and interaction cues improved performance to F1 = <strong>85%</strong>, F2 = <strong>87.63%</strong>, AUC-ROC = <strong>86.47</strong> (SVM-RBF).</li> <li><strong>EHR data (structured data + clinical notes):</strong> Logistic Regression reached F1 = <strong>75.56%</strong>, F2 = <strong>83.33%</strong>, AUC-ROC = <strong>79.70</strong>.</li> <li><strong>Combined communication + EHR:</strong> Late-fusion SVM-Linear achieved the best performance with F1 = <strong>88.89%</strong>, F2 = <strong>86.02%</strong>, and AUC-ROC = <strong>90.23</strong>.</li> </ul> These results show that integrating routine patient–nurse verbal communication with EHR data substantially enhances early detection of cognitive decline in home healthcare settings."
  },
  {
    "title": "Explainability",
    "authors": "Maryam Zolnoori",
    "year": 2026,
    "type": "In Preparation",
    "image": "",
    "challenge": "…",
    "solution": "…",
    "result": "…"
  }
]